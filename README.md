# MNIST-Image-Classification-with-Custom-MLP

## Overview

This project focuses on training a Multi-Layer Perceptron (MLP) for MNIST digit classification, exclusively using the numpy library. The primary goal is to delve deep into backpropagation and neural network training principles, bypassing higher-level frameworks like TensorFlow and PyTorch.

## Key Components

- Customizable MLP architecture with variable hidden layers, activation functions (tanh/ReLU), and neuron counts.
- Utilizes the cross-entropy cost function and employs Stochastic Gradient Descent (SGD) optimization.
- Incorporates learning rate decay, regularization techniques, and flexible parameter initialization.
- Evaluation based on validation accuracy and final testing against the MNIST dataset.
- Encourages experimentation with various configurations to achieve high accuracy.

## Project Objectives

- Manual implementation of an MLP for MNIST digit classification.
- Extensive hyperparameter exploration for optimal model performance.
- Utilizes learning rate decay to fine-tune training.
- Aims for a test accuracy within the range of 95% to 99%.
